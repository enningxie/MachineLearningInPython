# Python 机器学习
## 预测分析核心算法
### 简介
本文主要介绍了机器学习算法的两大类：惩罚线性回归（岭回归、Lasso算法）；集成方法（随机森林、梯度提升算法）。  

惩罚线性回归代表了对最小二乘法回归方法的相对较新的改善和提高。惩罚线性回归具有的几个特征使其成为预测分析的首选，惩罚线性回归引入了一个可调参数，使最终的模型在过拟合与欠拟合之间达到了平衡。  

集成方法是目前最有力的预测分析工具之一。它可以对特别复杂的行为进行建模，特别是过定的问题，由于集成方法的性能，许多经验丰富的数据科学家在做第一次尝试时都使用该方法。集成方法使用相对简单，而且可以依据对预测的贡献程度对特征排序。  

目前集成方法与惩罚线性回归齐头并进，惩罚线性回归是从克服一般回归方法的局限性进化而来的，集成方法是从克服二元决策树的局限性进化而来的。

### 第一章 关于预测的两类核心算法
本书集中于机器学习领域，只关注那些最有效和获得广泛使用的算法。

函数逼近问题是有监督学习问题的一个子集。

解决函数逼近问题的两类算法：惩罚线性回归和集成方法。

面对实践中遇到的绝大多数预测分析（函数逼近）问题，这两类算法都具有最优或接近最优的性能。

当数据含有大量的特征时，但没有足够多的数据或时间来训练更复杂的集成方法模型时，惩罚回归方法将优于其他算法。

惩罚线性回归模型一个重要优势就是它训练所需时间（训练时间快），部署已训练好的模型后进行预测的时间也特别快。

研究表明惩罚线性回归在许多情况下可以提供最佳的答案，在即使不是最佳答案的情况下，也可以提供接近最佳的答案。

在预测模型构建过程中，最耗时的一步就是特征提取或者叫做特征工程。

惩罚线性回归方法是由普通最小二乘法衍生出来的，其设计之初的想法就是克服最小二乘法的根本缺陷，最小二乘法的一个根本问题就是有时它会过拟合。

自由度：统计学上的自由度是指当以样本的统计量来估计总体的参数时，样本中独立或能自由变化的自变量的个数称为该统计量的自由度。

惩罚线性回归可以减少自由度使之与数据规模、问题的复杂度相匹配。对于具有大量自由度的问题，惩罚线性回归方法获得了广泛的应用。

集成方法的基本思想是构建多个不同的预测模型，然后将其输出做某种组合作为最终的输出，如取平均值或采用多数人的意见（投票）。

由于某些机器学习算法输出结果不稳定，这一问题导致了集成方法的提出。

即使在某些情况下，惩罚线性回归的性能不如集成方法，它也可以是建立一个机器学习系统的有意义的第一步尝试。

如果一个问题不是很复杂，而且不能获得足够多的数据，则线性方法比更加复杂的集成方法可能会获得全面更优的性能。

线性模型倾向于训练速度快，并且经常能够提供与非线性集成方法相当的性能，特别是当能获取的数据受限时。因为它们训练时间短，在早期特征选取阶段训练线性模型是很方便的，然后可以据此大致估计针对特定问题可以达到的性能。线性模型可以提供关于特征对预测的相关信息，可以辅助特征选取阶段的工作。在有充足数据的情况下，集成方法通常能提供更好的性能，也可以提供相对间接的关于结果的贡献的评估。

构建预测模型的流程：
- 提取或组合预测所需的特征
- 设定训练目标
- 训练模型
- 评估模型在测试数据上的性能表现

特征提取就是一个把自由形式的各种数据转换成行、列形式的数字的过程。

特征工程就是对特征进行整理组合，以达到更富有信息量的过程。

### 第二章 通过理解数据来了解问题
